---
title: "P8105_hw6_qh2251"
author: "Paulina Han"
date: "2021/11/25"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

library(modelr)
library(mgcv)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

Read the data

```{r}
#raw data
df_raw = read.csv("birthweight.csv", na = c("", "NA", "Unknown"))

#clean version
df_tidy = df_raw %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace),
         ) 

#check missing values  
sum(is.na(df_tidy))

```

The data set contains `r nrow(df_raw)` observations and `r ncol(df_raw)` variables.I converted `babysex`, `frace`, `malform` and `mrace` into factors. There are no missing values in the data set.

# Regression

To predict the value of birth weight，I include `wtgain`:mother's total weight gain, `smoken`: average number of cigarettes smoked per day during pregnancy , `parity` : number of live births prior to this pregnancy, `babysex` : baby’s sex (male = 1, female = 2), `ppbmi` : mother’s pre-pregnancy BMI, `momage` : mother’s age at delivery (years) in a linear model. I picked out these predictors according to a study done by Kari Johansson in 2007. Here is the [link](https://www.sciencedirect.com/science/article/pii/S1871403X07000622?via%3Dihub) to their interesting paper in predicting child's birth weight.

We can see from the "Predicted vs. Residuals" plot that the average residuals is approximately 0 and the average predicted is around 3000.

```{r}
#fitting model from paper:DOI: 10.1016/j.orcp.2007.09.001
# wtgain:total weight gain
# smoken: average number of cigarettes smoked per day during pregnancy
# parity: number of live births prior to this pregnancy
# babysex: baby’s sex (male = 1, female = 2)
# ppbmi: mother’s pre-pregnancy BMI
# momage: mother’s age at delivery (years)

#model 1 
lm1 = lm(bwt ~ wtgain + smoken + parity + babysex + ppbmi + momage, data = df_tidy)

#summary(lm1)

broom::tidy(lm1) %>% 
  knitr::kable(digits = 3)

#plot 
df_tidy %>% 
  add_predictions(lm1) %>% 
  add_residuals(lm1) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5)+
  labs(
    title = "Predicted vs. Residuals",
    x = "Predicted",
    y = "Residuals"
    ) 
```

# Model Comparison

The violin plot illustrates the root-mean-square error (RMSE) across models.We can see from the plot that model 1 is having largest rmse and model 3 has the smallest rmse which implies if we want to have more accurate prediction of child's birth weight we should use model 3 comparing with model 1 and 2.

```{r}
#model2: using length at birth and gestational age as predictors (main effects only)
m2 = lm(bwt ~ blength + gaweeks, data = df_tidy) 
broom::tidy(m2)

#model3:using head circumference, length, sex, and all interactions (including the three-way interaction) between these
m3 = lm(bwt ~ bhead * blength * babysex, data = df_tidy) 
broom::tidy(m3)


#cross validation
#split in to train and test
cv_df =
  crossv_mc(df_tidy, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

#calculating rmse
cv_df = 
  cv_df %>% 
  mutate(
    model1  = map(train, ~lm(bwt ~ wtgain + smoken + parity + babysex + ppbmi + momage, data = .x)),
    model2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model3  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = as_tibble(.x)))) %>% 
  mutate(
    rmse_m1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_m2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
    rmse_m3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))

#result comparison
result =  cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_")  %>% 
  mutate(model = fct_inorder(model)) 

# violin plot of rmse
result%>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()+
  labs(
    title = "model comparison",
    x = "fitted models",
    y = "RMSE"
    ) 

```



# Problem 2

```{r}
#load the data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

#boot strap df
set.seed(1)

bs_df = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance),
    betas = map(models, broom::tidy)
  ) %>% 
    select(strap_number, results ,betas) %>% 
  unnest(results,betas) %>% 
  select(strap_number, r.squared, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  rename(
    beta_0 = "(Intercept)",
    beta_1 = tmin
  ) %>% 
 mutate(
    log_fx = log(beta_0*beta_1)
  )

```


plot the distribution:

```{r}
#plot r^2
bs_df %>%
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
        title = "Distribution of R Squared",
        x = "R Squared"
    ) 

```

The distribution of $\hat{r}^{2}$ is left-skewed bell-shaped curve. The distribution peaks at approximately 0.91.

```{r}
#plot log(beta0*beta1)
bs_df %>%
  ggplot(aes(x = log_fx)) +
  geom_density() +
  labs(
        title = "Distribution of Log Function",
        x = "Log Function"
    ) 
```

The distribution of the log(beta0 * beta1) is bell-shaped curve. The distribution peaks at approximately 2.01.


```{r}
#CI-band
bs_df %>% 
  select(r.squared, log_fx) %>% 
  pivot_longer(
    r.squared:log_fx,
    names_to = "term",
    values_to = "estimate"
  ) %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  ) %>% 
  knitr::kable(digits = 2)

```

We can see from above the 95% CI-Band for $\hat{r}^{2}$ is (0.89,0.93) according to bootstrap. The 95% CI-Band for the log function is (1.96,2.06) according to bootstrap. 


